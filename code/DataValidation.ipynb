{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9503568d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sdv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d28ae2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sdv.single_table import GaussianCopulaSynthesizer\n",
    "from sdv.sampling import Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc42a8f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrafficDataDC:\n",
    "    pass\n",
    "\n",
    "with open('./TrafficDataDC.pickle', 'rb') as f:\n",
    "    s3_traffic = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2bbdc4-8c9e-40ea-981a-fd35c7df8e64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_traffic.df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3079e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_percentage = 1\n",
    "split_index = int(len(s3_traffic.df) * train_percentage)\n",
    "train_data = s3_traffic.df.iloc[:split_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32caf7c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad99578",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata = SingleTableMetadata()\n",
    "metadata.detect_from_dataframe(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a363ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata.update_column(\n",
    "    column_name='container_id',\n",
    "    sdtype='categorical')\n",
    "\n",
    "metadata.update_column(\n",
    "    column_name='chunk_id',\n",
    "    sdtype='categorical')\n",
    "\n",
    "metadata.update_column(\n",
    "    column_name='disk_capacity_tb',\n",
    "    sdtype='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3762e48-b64b-45de-bb2f-63c86baae458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a19b66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "synthesizer = GaussianCopulaSynthesizer(metadata,\n",
    "                                        enforce_min_max_values=False,\n",
    "                                        enforce_rounding=True,\n",
    "                                        locales='en_US'\n",
    "                                       )\n",
    "\n",
    "synthesizer.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a5ee53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1872a7-5c03-4695-97d4-59cb86a7da90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataModel:\n",
    "    def predict(self, day, month, disk_capacity, container_group):\n",
    "        container_group_encoded = self.label_encoder.transform([container_group])[0]\n",
    "        # Prepare input for prediction\n",
    "        sample_input = pd.DataFrame({\n",
    "            'disk_capacity_tb': [disk_capacity],\n",
    "            'container_group_encoded': [container_group_encoded],\n",
    "            'month': [month],\n",
    "            'day': [day],\n",
    "            'day_sin': [np.sin(day * (2. * np.pi / 31))],\n",
    "            'day_cos': [np.cos(day * (2. * np.pi / 31))],\n",
    "            'month_sin': [np.sin((month - 1) * (2. * np.pi / 12))],\n",
    "            'month_cos': [np.cos((month - 1) * (2. * np.pi / 12))]\n",
    "        })\n",
    "\n",
    "        return int(self.model.predict(sample_input)[0])\n",
    "    pass\n",
    "with open('SamplePredictor.pickle', 'rb') as f:\n",
    "    data_model = pickle.load(f)\n",
    "    # print(data_model.predict(day=1, month=1, disk_capacity=20, container_group='02892102A8F17B5A551466B444222F4C3D9A399F'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726a92ac-5c9a-446c-b9d5-08a1d1d24c11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped_data = train_data.groupby('container_group')['container_id'].agg(list).reset_index()\n",
    "\n",
    "container_dict = dict(zip(grouped_data['container_group'], grouped_data['container_id']))\n",
    "\n",
    "def generate_conditions(start_date, end_date, disk_capacity, container_groups):\n",
    "    conditions = []\n",
    "    current_date = start_date\n",
    "    delta = timedelta(days=1)\n",
    "    while current_date <= end_date:\n",
    "        for container_group, container_id_types in container_groups.items():\n",
    "            container_ids = random.sample(container_dict[container_group], container_id_types)\n",
    "            num_rows = data_model.predict(day=current_date.day, month=current_date.month, disk_capacity=disk_capacity, container_group=container_group)\n",
    "            conditions.extend([Condition(num_rows=int(num_rows/len(container_ids)), \n",
    "                                         column_values={'datetime_column': current_date, \n",
    "                                                        'container_group': container_group,\n",
    "                                                        'container_id': container_id\n",
    "                                                       }\n",
    "                                        )\n",
    "                               for container_id in container_ids])\n",
    "        current_date += delta\n",
    "    return conditions\n",
    "\n",
    "def generate_conditions(start_date, end_date, disk_capacity, container_groups):\n",
    "    conditions = []\n",
    "    \n",
    "    total_samples = 0\n",
    "    \n",
    "    current_date = start_date\n",
    "    delta = timedelta(days=1)\n",
    "\n",
    "    for container_group, container_id_types in container_groups.items():\n",
    "        container_ids = random.sample(container_dict[container_group], container_id_types)\n",
    "        current_date = start_date\n",
    "        while current_date <= end_date:\n",
    "\n",
    "            num_rows = data_model.predict(day=current_date.day, month=current_date.month, disk_capacity= disk_capacity, container_group= container_group)\n",
    "            total_samples += num_rows\n",
    "            print(container_ids)\n",
    "            print({'datetime_column': current_date, \n",
    "                                                        'disk_capacity_tb': disk_capacity_tb, \n",
    "                                                        'container_group': container_group, 'num_rows':int(num_rows/len(container_ids))})\n",
    "            conditions.extend([Condition(num_rows=int(num_rows/len(container_ids)), \n",
    "                                         column_values={'datetime': current_date, \n",
    "                                                        'container_group': container_group,\n",
    "                                                        'container_id': container_id\n",
    "                                                       }\n",
    "                                        )\n",
    "                               for container_id in container_ids])\n",
    "\n",
    "            current_date += delta\n",
    "    print(total_samples)\n",
    "    return conditions\n",
    "\n",
    "# defining variables to be passed to the synthesizer\n",
    "start_date = datetime.strptime('2022-01-27', '%Y-%m-%d')\n",
    "end_date = datetime.strptime('2022-01-28', '%Y-%m-%d')\n",
    "disk_capacity_tb = 16\n",
    "# this translates to I want 2 container_ids of type X and 3 container_ids of type Y\n",
    "container_groups = {'02892102A8F17B5A551466B444222F4C3D9A399F':2, 'CC21F742BC91C1A0ED11A719D5C2CE74690BCD44':3}\n",
    "\n",
    "conditions = generate_conditions(start_date, end_date, disk_capacity_tb, container_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0ce8c0-b89c-4f23-b4f2-8c183dc24a64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csd = synthesizer.sample_from_conditions(conditions=conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed0b7cb-920f-41e8-940f-963aa7c82ed5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d8f567-4a74-42ba-beed-40196c55ac57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(csd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35bc1ab-5ffe-4031-9dab-d951d2d07b28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csd.groupby('container_id').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7ab98e-8207-47ee-a566-05fd177c03ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp = csd.groupby(['container_group', 'container_id'])['chunk_id'].count().reset_index()\n",
    "tr = dict(temp.groupby('container_group')['chunk_id'].sum())\n",
    "\n",
    "\n",
    "print(train_data[train_data['container_id'] == container]['location_id'].unique())\n",
    "print(train_data[train_data['container_id'] == container]['server_id'].unique())\n",
    "print(train_data[train_data['container_id'] == container]['config_id'].unique())\n",
    "print(type(train_data[train_data['container_id'] == container]['disk_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d612279-348b-413e-8b9d-686621a8ce0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csd['disk_capacity_tb'] = disk_capacity_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b128a3-904c-4dff-b39c-75381f2bfa9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1631864-851f-414a-8170-d09cdfb94007",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataQValidation:\n",
    "\n",
    "    def __init__(self, original, synthetic, start_date, end_date, disk_capacity_tb, container_groups):\n",
    "        self.original_df = original\n",
    "        self.synthetic_df = synthetic\n",
    "\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.disk_cap = disk_capacity_tb\n",
    "        self.container_groups = container_groups\n",
    "\n",
    "        self.original_df['datetime'] = pd.to_datetime(self.original_df['datetime'], errors='coerce')\n",
    "        self.synthetic_df['datetime'] = pd.to_datetime(self.synthetic_df['datetime'], errors='coerce')\n",
    "\n",
    "        self.original_df = self.original_df.sort_values(by='datetime')\n",
    "        self.synthetic_df = self.synthetic_df.sort_values(by='datetime')\n",
    "\n",
    "    def unique_data(self):\n",
    "\n",
    "        def row_hash(row):\n",
    "            return hashlib.sha256(row.to_string().encode('utf-8')).hexdigest()\n",
    "\n",
    "        original_hashes = self.original_df.apply(lambda row: row_hash(row), axis=1)\n",
    "        synthetic_hashes = self.synthetic_df.apply(lambda row: row_hash(row), axis=1)\n",
    "\n",
    "        tqdm.pandas(desc=\"Hashing original rows\")\n",
    "        original_hashes = self.original_df.progress_apply(row_hash, axis=1)\n",
    "\n",
    "        tqdm.pandas(desc=\"Hashing synthetic rows\")\n",
    "        synthetic_hashes = self.synthetic_df.progress_apply(row_hash, axis=1)\n",
    "\n",
    "        original_set = set(original_hashes)\n",
    "        synthetic_set = set(synthetic_hashes)\n",
    "\n",
    "        exact_matches = original_set.intersection(synthetic_set)\n",
    "\n",
    "        print(\"Number of exact matches with original data:\", len(exact_matches))\n",
    "\n",
    "        print(\"Number of exact matches within synthetic data:\", len(synthetic_hashes) - len(synthetic_set))\n",
    "\n",
    "    def check_capacity(self):\n",
    "\n",
    "        cap_cumulative_test = True\n",
    "        positive_chunk = True\n",
    "\n",
    "        cumulative_sizes = {container_id: 0 for container_id in self.synthetic_df['container_id'].unique()}\n",
    "        fixed_disk_cap_gb = 16 * 1e9\n",
    "\n",
    "        for index, row in self.synthetic_df.iterrows():\n",
    "\n",
    "            if row['chunk_size'] < 0:\n",
    "                positive_chunk = False\n",
    "\n",
    "            if row['operation'] == 'WRITE':\n",
    "                cumulative_sizes[row['container_id']] += row['chunk_size']\n",
    "\n",
    "            if row['operation'] == 'DELETE_PERFORMED':\n",
    "                cumulative_sizes[row['container_id']] -= row['chunk_size']\n",
    "\n",
    "            if cumulative_sizes[row['container_id']] > fixed_disk_cap_gb: # Assuming we get 100% of the theoretical space\n",
    "                cap_cumulative_test = False\n",
    "\n",
    "        if cap_cumulative_test:\n",
    "            print(\"Cumulative Data Test Passed : Size did not exceed disk capacity for any container at any given time\")\n",
    "        else:\n",
    "            print(\"Cumulative Data Test Failed : Size did exceeded disk capacity for some container at a given time\")\n",
    "\n",
    "        if positive_chunk:\n",
    "            print(\"Positive Chunk Size Test Passed\")\n",
    "        else:\n",
    "            print(\"Positive Chunk Size Test Failed\")\n",
    "            \n",
    "        print(cumulative_sizes)\n",
    "        print(fixed_disk_cap_gb)\n",
    "\n",
    "    def null_check(self):\n",
    "        if self.synthetic_df.isnull().values.any():\n",
    "            print(\"Null Test Failed : Null values found in the generated data\")\n",
    "        else:\n",
    "            print(\"Null Test Passed\")\n",
    "\n",
    "    def range_check(self):\n",
    "\n",
    "        COUNT_THRESHOLD = 3\n",
    "\n",
    "        container_group_master = self.original_df['container_group'].unique()\n",
    "\n",
    "        operations = set(self.synthetic_df['operation'])\n",
    "        ops_master = set(self.original_df['operation'])\n",
    "        if not operations.issubset(ops_master):\n",
    "            print('Operation Range Test Failed : Unkown Operation found')\n",
    "        else:\n",
    "            print(f'Operation Range Test Passed : Unique operations found in synthetic dataset are\\n{operations}')\n",
    "\n",
    "        print('\\n')\n",
    "        if self.disk_cap:\n",
    "            disk_caps = set(self.synthetic_df['disk_capacity_tb'])\n",
    "            if len(disk_caps) == 1:\n",
    "                if next(iter(disk_caps)) == self.disk_cap:\n",
    "                    print(\"Disk Capacity Range Test Passed : Synthetic data generated for the conditioned disk capacity\")\n",
    "                else:\n",
    "                    print(\"Disk Capacity Range Test Failed : Synthetic data generated has different disk capacity than the condition\")\n",
    "            else:\n",
    "                print(\"Disk Capacity Range Test Failed : 0 or more than 1 unique disk capacities found in synthetic data\")\n",
    "\n",
    "        print('\\n')\n",
    "        if self.container_groups:\n",
    "            if not set(self.container_groups.keys()).issubset(container_group_master):\n",
    "                print(\"Container Group Range Test cannot be perfomed, synthetic data generated on different container groups than original data\")\n",
    "            else:\n",
    "                synthetic_cg_master = self.synthetic_df['container_group'].unique()\n",
    "                if not set(self.container_groups.keys()).issubset(synthetic_cg_master):\n",
    "                    print(\"Container Group Range Test Failed : synthetic data is missing some conatiner groups\")\n",
    "                elif not set(synthetic_cg_master).issubset(self.container_groups.keys()):\n",
    "                    print(\"Container Group Range Test Failed : synthetic data has extra conatiner groups not passed in condition\")\n",
    "                else:\n",
    "                    print(\"Container Group Range Test Passed : synthetic data and conditions have exact container groups\")\n",
    "\n",
    "        synthetic_cg_metadf = self.synthetic_df.groupby(['container_group', 'container_id'])['chunk_id'].count().reset_index()\n",
    "        transaction_counts = dict(synthetic_cg_metadf.groupby('container_group')['chunk_id'].sum())\n",
    "        container_counts = dict(synthetic_cg_metadf.groupby('container_group')['container_id'].count())\n",
    "        \n",
    "        predictor = self.load_predictor_model()\n",
    "        delta = timedelta(days=1)\n",
    "\n",
    "        transactions_test = True\n",
    "        container_count_test = True\n",
    "        total_transactions = 0\n",
    "        for cg, ccount in self.container_groups.items():\n",
    "            current_date = self.start_date\n",
    "            predicted_count = 0\n",
    "            while current_date <= end_date:\n",
    "                predicted_count += predictor.predict(day=current_date.day, month=current_date.month, disk_capacity= self.disk_cap, container_group= cg)\n",
    "                current_date += delta\n",
    "\n",
    "            if abs(predicted_count - transaction_counts[cg]) > COUNT_THRESHOLD:\n",
    "                transactions_test = False\n",
    "            \n",
    "            # print(container_counts[cg])\n",
    "            # print(ccount)\n",
    "            if ccount != container_counts[cg]:\n",
    "                container_count_test = False\n",
    "\n",
    "        print('\\n')\n",
    "        if transactions_test:\n",
    "            print('Transaction Number Test Passed : Number of samples generated is in lieu with models prediction')\n",
    "        else:\n",
    "            print('Transaction Number Test Failed : Number of samples generated is not in lieu with models prediction')\n",
    "\n",
    "        print('\\n')\n",
    "        if container_count_test:\n",
    "            print('Container Count Test Passed : Number of unique containers found in synthetic data is equal to condition')\n",
    "        else:\n",
    "            print('Container Count Test Failed : Number of unique containers found in synthetic data is not equal to condition')\n",
    "\n",
    "        containers = list(synthetic_cg_metadf['container_id'])\n",
    "\n",
    "        lid_test = True\n",
    "        sid_test = True\n",
    "        cid_test = True\n",
    "        did_test = True\n",
    "\n",
    "        for co in containers:\n",
    "            uog = self.original_df[self.original_df['container_id'] == co]\n",
    "            uog_lid = set(uog['location_id'].unique())\n",
    "            uog_sid = set(uog['server_id'].unique())\n",
    "            uog_cid = set(uog['config_id'].unique())\n",
    "            uog_did = set(uog['disk_id'].unique())\n",
    "\n",
    "            us = self.synthetic_df[self.synthetic_df['container_id'] == co]\n",
    "            us_lid = set(us['location_id'].unique())\n",
    "            us_sid = set(us['server_id'].unique())\n",
    "            us_cid = set(us['config_id'].unique())\n",
    "            us_did = set(us['disk_id'].unique())\n",
    "\n",
    "            if not us_lid.issubset(uog_lid):\n",
    "                lid_test = False\n",
    "            if not us_sid.issubset(uog_sid):\n",
    "                sid_test = False\n",
    "            if not us_cid.issubset(uog_cid):\n",
    "                cid_test = False\n",
    "            if not us_did.issubset(uog_did):\n",
    "                did_test = False\n",
    "\n",
    "        print('\\n')\n",
    "        if lid_test:\n",
    "            print('Location Uniqueness Test Passed : Container in synthetic data and original data belong to same location id')\n",
    "        else:\n",
    "            print('Location Uniqueness Test Failed : Container in synthetic data and original data do not belong to same location id')\n",
    "\n",
    "        print('\\n')\n",
    "        if sid_test:\n",
    "            print('Server Uniqueness Test Passed : Container in synthetic data and original data belong to same server id')\n",
    "        else:\n",
    "            print('Server Uniqueness Test Failed : Container in synthetic data and original data do not belong to same server id')\n",
    "\n",
    "        print('\\n')\n",
    "        if cid_test:\n",
    "            print('Config Uniqueness Test Passed : Container in synthetic data and original data belong to same config id')\n",
    "        else:\n",
    "            print('Config Uniqueness Test Failed : Container in synthetic data and original data do not belong to same config id')\n",
    "\n",
    "        print('\\n')\n",
    "        if did_test:\n",
    "            print('Disk Uniqueness Test Passed : Container in synthetic data and original data belong to same disk id')\n",
    "        else:\n",
    "            print('Disk Uniqueness Test Failed : Container in synthetic data and original data do not belong to same disk id')\n",
    "\n",
    "        outside_range = self.synthetic_df[(self.synthetic_df['datetime'] < self.start_date) | (self.synthetic_df['datetime'] > self.end_date)]\n",
    "\n",
    "        print('\\n')\n",
    "        if outside_range.empty:\n",
    "            print('Date Range Test Passed : All dates are within the provided start and end date')\n",
    "        else:\n",
    "            print('Date Range Test Failed : Some dates are outside start and end date')\n",
    "\n",
    "    def load_predictor_model(self):\n",
    "        class DataModel:\n",
    "            def predict(self, day, month, disk_capacity, container_group):\n",
    "                container_group_encoded = self.label_encoder.transform([container_group])[0]\n",
    "                # Prepare input for prediction\n",
    "                sample_input = pd.DataFrame({\n",
    "                    'disk_capacity_tb': [disk_capacity],\n",
    "                    'container_group_encoded': [container_group_encoded],\n",
    "                    'month': [month],\n",
    "                    'day': [day],\n",
    "                    'day_sin': [np.sin(day * (2. * np.pi / 31))],\n",
    "                    'day_cos': [np.cos(day * (2. * np.pi / 31))],\n",
    "                    'month_sin': [np.sin((month - 1) * (2. * np.pi / 12))],\n",
    "                    'month_cos': [np.cos((month - 1) * (2. * np.pi / 12))]\n",
    "                })\n",
    "\n",
    "                return int(self.model.predict(sample_input)[0])\n",
    "            pass\n",
    "        with open('SamplePredictor.pickle', 'rb') as f:\n",
    "            data_model = pickle.load(f)\n",
    "        return data_model\n",
    "\n",
    "    def stat_dist(self, col, isCat=False):\n",
    "        if isCat:\n",
    "            print(f\"Original {col} distribution:\\n\", self.original_df[col].value_counts(normalize=True))\n",
    "            print(f\"Synthetic {col} distribution:\\n\", self.synthetic_df[col].value_counts(normalize=True))\n",
    "        else:\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "            # Plotting histograms\n",
    "            sns.histplot(self.original_df[col], color=\"skyblue\", label='Original', kde=True, ax=ax)\n",
    "            sns.histplot(self.synthetic_df[col], color=\"red\", label='Synthetic', kde=True, ax=ax, alpha=0.6)\n",
    "\n",
    "            plt.legend()\n",
    "            plt.title(f'Distribution of {col}')\n",
    "\n",
    "            # Summary statistics\n",
    "            original_stats = self.original_df[col].describe()\n",
    "            synthetic_stats = self.synthetic_df[col].describe()\n",
    "\n",
    "            stats_df = pd.DataFrame({'Original': original_stats, 'Synthetic': synthetic_stats})\n",
    "            stats_text = stats_df.to_string()\n",
    "\n",
    "            # Adding text box for summary statistics\n",
    "            plt.text(1.05, 0.95, stats_text, transform=ax.transAxes, fontsize=10, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "    def unique_data_by_col(self, col):\n",
    "        print(f\"Unique values in original data {col}:\", self.original_df[col].nunique())\n",
    "        print(f\"Unique values in synthetic data {col}:\", self.synthetic_df[col].nunique())\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_datetime(df):\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "        return df\n",
    "\n",
    "    def compare_numerical_distributions(self, col):\n",
    "        \"\"\"\n",
    "        Compare the distributions of a numerical column using histograms and the Kolmogorov-Smirnov test.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.histplot(self.original_df[col], color=\"skyblue\", label=\"Original\", kde=True, stat=\"density\", bins=30)\n",
    "        sns.histplot(self.synthetic_df[col], color=\"red\", label=\"Synthetic\", kde=True, stat=\"density\", bins=30)\n",
    "        plt.legend()\n",
    "        plt.title(f'Distribution Comparison for {col}')\n",
    "        plt.show()\n",
    "\n",
    "        stat, p = ks_2samp(self.original_df[col], self.synthetic_df[col])\n",
    "        print(f\"Kolmogorov-Smirnov test for {col}: Statistic={stat:.4f}, P-value={p:.4g}\")\n",
    "\n",
    "    def compare_categorical_distributions(self, col):\n",
    "        \"\"\"\n",
    "        Compare the distributions of a categorical column using count plots on the same graph\n",
    "        for both original and synthetic datasets.\n",
    "        :param col: The column name for the categorical data.\n",
    "        \"\"\"\n",
    "        original_df_copy = self.original_df.copy()\n",
    "        synthetic_df_copy = self.synthetic_df.copy()\n",
    "        original_df_copy['Dataset'] = 'Original'\n",
    "        synthetic_df_copy['Dataset'] = 'Synthetic'\n",
    "\n",
    "        combined_df = pd.concat([original_df_copy, synthetic_df_copy], ignore_index=True)\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.countplot(x=col, hue='Dataset', data=combined_df, palette='viridis')\n",
    "        plt.title(f'Comparison of {col} Distribution Between Original and Synthetic Data')\n",
    "        plt.xticks(rotation=45)  # Rotate the x-axis labels for better readability\n",
    "        plt.legend(title='Dataset')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def temporal_distribution_check(self):\n",
    "        \"\"\"\n",
    "        Validate the consistency of event distribution over time.\n",
    "        \"\"\"\n",
    "        original_timeseries = self.original_df.set_index('datetime').resample('M').size()\n",
    "        synthetic_timeseries = self.synthetic_df.set_index('datetime').resample('M').size()\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        original_timeseries.plot(label='Original', color='blue')\n",
    "        synthetic_timeseries.plot(label='Synthetic', color='red')\n",
    "        plt.legend()\n",
    "        plt.title('Temporal Distribution Comparison')\n",
    "        plt.xlabel('Month')\n",
    "        plt.ylabel('Count')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_cdf(self, col):\n",
    "        \"\"\"\n",
    "        Plots the CDF for a numerical column for both original and synthetic datasets.\n",
    "        :param col: The column name to analyze.\n",
    "        \"\"\"\n",
    "        x_original = np.sort(self.original_df[col])\n",
    "        y_original = np.arange(1, len(x_original)+1) / len(x_original)\n",
    "        x_synthetic = np.sort(self.synthetic_df[col])\n",
    "        y_synthetic = np.arange(1, len(x_synthetic)+1) / len(x_synthetic)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(x_original, y_original, marker='.', linestyle='none', label='Original')\n",
    "        plt.plot(x_synthetic, y_synthetic, marker='.', linestyle='none', label='Synthetic')\n",
    "        plt.legend()\n",
    "        plt.title(f'Cumulative Distribution Function (CDF) of {col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('CDF')\n",
    "        plt.show()\n",
    "\n",
    "    def run_test_suite(self):\n",
    "        self.null_check()\n",
    "        print('\\n')\n",
    "        print('-'*50)\n",
    "        self.stat_dist(col='chunk_size')\n",
    "        print('\\n')\n",
    "        print('-'*50)\n",
    "        self.unique_data()\n",
    "        print('\\n')\n",
    "        print('-'*50)\n",
    "        self.range_check()\n",
    "        print('\\n')\n",
    "        print('-'*50)\n",
    "        self.check_capacity()\n",
    "        print('\\n')\n",
    "        print('-'*50)\n",
    "        self.compare_numerical_distributions(col='chunk_size')\n",
    "        print('\\n')\n",
    "        print('-'*50)\n",
    "        self.plot_cdf(col='chunk_size')\n",
    "        print('\\n')\n",
    "        print('-'*50)\n",
    "        self.temporal_distribution_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7fd4f6-164b-49df-9bde-6c194651816a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation = DataQValidation(train_data, csd, start_date, end_date, disk_capacity_tb, container_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3249d6db-e6f7-44d1-899b-003f9fc38814",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation.range_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d31c76-18fe-4526-8f4c-ae037641dc2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation.run_test_suite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d97079-685e-411a-8063-0ee0f88b854a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "containers = list(csd['container_id'].unique())\n",
    "\n",
    "for co in containers:\n",
    "    print(co)\n",
    "    uog = train_data[train_data['container_id'] == co]\n",
    "    uog_lid = set(uog['server_id'].unique())\n",
    "    print(uog_lid)\n",
    "    print()\n",
    "    us = csd[csd['container_id'] == co]\n",
    "    us_lid = set(us['server_id'].unique())\n",
    "    print(us_lid)\n",
    "    print(us_lid.issubset(uog_lid))\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e23409a-6b9b-45b3-be11-e8d01ab7fbcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation.check_capacity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db49c563-eba0-45f0-9e57-bfd3e567f358",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation.stat_dist(col='chunk_size')\n",
    "print(\"------------------------------\")\n",
    "validation.unique_data_by_col(col='container_id')\n",
    "print(\"------------------------------\")\n",
    "validation.unique_data_by_col(col='container_group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eb09e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.unique_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b92393d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation.check_capacity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3cbff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.compare_categorical_distributions('operation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b5c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.temporal_distribution_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99de47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.compare_numerical_distributions('chunk_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bec310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.gofplots import qqplot\n",
    "qqplot(train_data['chunk_size'], line='s')\n",
    "plt.title('Q-Q Plot of chunk_size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4a3795",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.plot_cdf('chunk_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de05df8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = train_data.groupby('container_group')['container_id'].agg(list).reset_index()\n",
    "container_dict = dict(zip(grouped_data['container_group'], grouped_data['container_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9fd1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_conditions(start_date, end_date, disk_capacity, container_groups, num_rows):\n",
    "    conditions = []\n",
    "    \n",
    "    current_date = start_date\n",
    "    delta = timedelta(days=1)\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        for container_group, container_id_types in container_groups.items():\n",
    "            container_ids = random.sample(container_dict[container_group], container_id_types)\n",
    "            \n",
    "            conditions.extend([Condition(num_rows=num_rows, \n",
    "                                         column_values={'datetime': current_date, \n",
    "                                                        'disk_capacity_tb': disk_capacity, \n",
    "                                                        'container_group': container_group,\n",
    "                                                        'container_id': container_id\n",
    "                                                       }\n",
    "                                        )\n",
    "                               for container_id in container_ids])\n",
    "\n",
    "        current_date += delta\n",
    "\n",
    "    return conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d51b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining variables to be passed to the synthesizer\n",
    "start_date = datetime.strptime('2022-02-01', '%Y-%m-%d')\n",
    "end_date = datetime.strptime('2022-02-02', '%Y-%m-%d')\n",
    "disk_capacity = 16\n",
    "# this translates to I want 2 container_ids of type X and 3 container_ids of type Y\n",
    "container_groups = {'02892102A8F17B5A551466B444222F4C3D9A399F':2, 'CC21F742BC91C1A0ED11A719D5C2CE74690BCD44':3}\n",
    "num_rows = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f23fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = generate_conditions(start_date, end_date, disk_capacity, container_groups, num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b11cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "csd = synthesizer.sample_from_conditions(conditions=conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb95efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "csd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0848a273",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_conditional = DataQValidation(s3_traffic.df, csd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5931f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_conditional.stat_dist(col='chunk_size')\n",
    "print(\"------------------------------\")\n",
    "validation_conditional.unique_data_by_col(col='container_id')\n",
    "print(\"------------------------------\")\n",
    "validation_conditional.unique_data_by_col(col='container_group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f545f84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_conditional.unique_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b796aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_conditional.check_capacity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502749ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_conditional.compare_categorical_distributions('operation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e1ff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_conditional.compare_numerical_distributions('chunk_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96f6307",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_conditional.plot_cdf('chunk_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0515bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
