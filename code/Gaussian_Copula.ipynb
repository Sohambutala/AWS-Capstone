{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8d95284-5716-4ac0-bf7f-03e3928b118d",
   "metadata": {},
   "source": [
    "## Gaussian Copula Synthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f15f7f-8931-4425-b3aa-8371ecc4a713",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyathena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8ac80d-3fd7-41c6-8ee1-d6d287685763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2c4586-8cce-4827-b3ca-bbc6cbd252b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyathena import connect\n",
    "from pyathena.pandas.cursor import PandasCursor\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe5c572-c7c4-4812-9c4a-32162ae344a6",
   "metadata": {},
   "source": [
    "### Querying the S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bc2381-5ed2-4720-a439-a091d8aab707",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_athena_query(query, print_out=False):\n",
    "    cursor = connect(\n",
    "        region_name='us-west-2',\n",
    "        work_group=\"primary\",\n",
    "        cursor_class=PandasCursor).cursor()\n",
    "\n",
    "    df = cursor.execute(query).as_pandas()\n",
    "\n",
    "    if print_out:\n",
    "        print(df.to_markdown(index=False))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0794f8af-275f-4077-95da-de6046e00f50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"select * from AwsDataCatalog.uwdatascience2023.full_harddrivetraffic limit 3000000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f45d594-3ca5-4f1c-a79d-b3105b6537e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "athena_df = run_athena_query(query, print_out=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971f0b78-3ee1-4a6c-8384-e37129888c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "athena_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459c9df8-e0fb-414b-81d9-d73da9e98c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f4797dd-1eda-43c8-90cc-7a884f5dc4b2",
   "metadata": {},
   "source": [
    "### Data cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2425ba35-a609-41a3-9b70-f70c0cc7952d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = athena_df.sort_values(by=['chunk_id', 'timestamp_nano'])\n",
    "df['container_group'] = df.groupby('chunk_id')['container_group'].ffill()\n",
    "df['container_encoding'] = df.groupby('chunk_id')['container_encoding'].ffill()\n",
    "df['chunk_size'] = df.groupby('chunk_id')['chunk_size'].ffill()\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02daca08-0ca1-4fe9-93c0-329ae8fc14a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['datetime_column'] = pd.to_datetime(df['timestamp_nano'], unit='ns')\n",
    "df.drop(columns=['timestamp_nano'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d66490-bf88-4837-a1d0-7b49ee70e8d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['chunk_size'] = df['chunk_size'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243ea727-5d3b-4a1f-9c20-1526cd684af9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc1066b-6b40-4df1-9bb7-8e6c5b54aaef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = ['timestamp_nano', 'location_id', 'server_id', 'config_id', 'disk_id', 'container_id', 'container_group', 'container_encoding', 'operation', 'chunk_id', 'chunk_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344130b7-1721-4586-a859-91d147a249bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_percentage = 1\n",
    "split_index = int(len(df) * train_percentage)\n",
    "train_data = df.iloc[:split_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6521d5b5-2836-4b7f-a7cf-c80655e42d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dropping month_end and datetime_column\n",
    "columns_to_delete = ['month_end']\n",
    "train_data.drop(columns=columns_to_delete, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ee3c35-30de-458d-96f2-1abb854a70db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by 'container_group' and aggregating 'container_id' into a list\n",
    "grouped_data = train_data.groupby('container_group')['container_id'].agg(list).reset_index()\n",
    "\n",
    "# Creating a dictionary from the grouped data\n",
    "container_dict = dict(zip(grouped_data['container_group'], grouped_data['container_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0efcee5-40b4-4e52-90ec-3aec9710b04a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract only the date part and keep the column name and datatype unchanged\n",
    "train_data['datetime_column'] = train_data['datetime_column'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Convert the 'datetime_column' to datetime format\n",
    "train_data['datetime_column'] = pd.to_datetime(train_data['datetime_column'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cdce04-3c76-49d2-997f-45e36ba0ba61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad9c9c0-3a4c-4360-918a-b49b96d347b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb7e62c0-a76c-4d42-b618-d485ed0ad971",
   "metadata": {},
   "source": [
    "### Metadata analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a2bcae-19de-4eed-bb21-7d4177b15087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sdv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646dc1af-a3d6-4c27-9e54-03c6700d38b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sdv.metadata import SingleTableMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734177a1-0f01-45f4-82dd-68aaeb098862",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata = SingleTableMetadata()\n",
    "metadata.detect_from_dataframe(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f8e3e-bbda-44d6-a6b7-d0cee74eb015",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata.update_column(\n",
    "    column_name='container_id',\n",
    "    sdtype='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42f8650-4661-4077-8882-3d52db44190f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata.update_column(\n",
    "    column_name='chunk_id',\n",
    "    sdtype='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dbed44-c979-4542-ae59-1f730841fa5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata.update_column(\n",
    "    column_name='disk_capacity_tb',\n",
    "    sdtype='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b9960e-cc0d-4989-8337-cd273b5ef61b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4d3f7d-7a86-4c3b-8e5d-ac330c6e5253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6ffac91-aeeb-4d0d-867a-d343a8a11bb8",
   "metadata": {},
   "source": [
    "### Raw synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2411a823-2817-4735-a67a-2fd199f1bf54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sdv.single_table import GaussianCopulaSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c832e000-96f6-4ae3-a836-60fe30b91a8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "synthesizer = GaussianCopulaSynthesizer(metadata,\n",
    "                                        enforce_min_max_values=False,\n",
    "                                        enforce_rounding=True,\n",
    "                                        locales='en_US'\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb41668-279e-4c4e-94b4-ce2e452200df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "synthesizer.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac139be-e731-4987-9d03-aa1b5d83b970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "synthetic_data = synthesizer.sample(num_rows=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66acd113-7db9-4465-be2d-4bbfc0be9829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "synthetic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7f5127-1815-4991-bfd1-3576355fddcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc59d680-b350-462a-8a3e-e0e44ca8ec3d",
   "metadata": {},
   "source": [
    "### Visual inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d728d6b-d171-4ee1-b32f-767b87880a62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data.groupby('container_group').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5c5f07-49b2-4e1b-8457-07d978679a14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "synthetic_data.groupby('container_group').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf72e84-9817-4f45-a221-829ae80611c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff20bfcd-a9b7-4e4c-aeae-9817f5404943",
   "metadata": {},
   "source": [
    "### Sanity checks\n",
    "This ensures all primary keys are unique and non-null, discrete values in the synthetic data must adhere to the same categories as the real data, all column names present in training data are present in the synthetic data as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193672e0-1b3b-44f2-9632-5f38df8d0237",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sdv.evaluation.single_table import run_diagnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e66c03-0cee-40de-aea5-286deabdac50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perform basic validity checks\n",
    "diagnostic = run_diagnostic(\n",
    "    real_data=train_data,\n",
    "    synthetic_data=synthetic_data,\n",
    "    metadata=metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f29f3d-5a14-4500-a5ce-4c4b98febc1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f362a7f8-b6af-43b0-9023-fcfd18afe943",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sdv.evaluation.single_table import get_column_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f891d5-9148-4cea-97b9-290590aee606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the data\n",
    "fig = get_column_plot(\n",
    "    real_data=train_data,\n",
    "    synthetic_data=synthetic_data,\n",
    "    metadata=metadata,\n",
    "    column_name='disk_capacity_tb'\n",
    ")\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c87b1d5-488e-42fd-bdd8-f33c9524cd6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the data\n",
    "fig = get_column_plot(\n",
    "    real_data=train_data,\n",
    "    synthetic_data=synthetic_data,\n",
    "    metadata=metadata,\n",
    "    column_name='container_group'\n",
    ")\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fcca73-4a9d-4cc9-b65d-c034ddf2c099",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the data\n",
    "fig_operation = get_column_plot(\n",
    "    real_data=train_data,\n",
    "    synthetic_data=synthetic_data,\n",
    "    metadata=metadata,\n",
    "    column_name='operation'\n",
    ")\n",
    "    \n",
    "fig_operation.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e7fda7-e3c2-4344-bc3a-9bcca9f7ec17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the data\n",
    "fig = get_column_plot(\n",
    "    real_data=train_data,\n",
    "    synthetic_data=synthetic_data,\n",
    "    metadata=metadata,\n",
    "    column_name='chunk_size'\n",
    ")\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04f0f3e-50a7-4812-82b2-a228ab264431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66e36651-0f67-473a-ac3e-bbf50c0facd6",
   "metadata": {},
   "source": [
    "### Data quality reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206a8a8a-5f82-4969-97f1-919fa8c49376",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sdmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8be155-2fb4-48b0-b1f1-4faa60f7db45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sdmetrics.reports.single_table import QualityReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4490fa91-45c9-491b-b6ef-12ae496d6342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "report = QualityReport()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1516c45-c944-4049-b1e7-69627d48f4ec",
   "metadata": {},
   "source": [
    "The below metrics are best compared between synthetic data (free of conditions) and the training data. The reason being that the conditional synthetic data has set parameters. For example, with disk capacity fixed at 16 TB, we can't expect the disk capacity column in the synthetic data to match the distrution of the corresponding column in the original data that has a range from 10-32 TB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dde9266-f107-4a25-8a70-33cb2e615500",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Column shape\n",
    "The shape of a column describes its overall distribution. The higher the score, the more similar the distributions of real and synthetic data\n",
    "\n",
    "#### Column pair trends\n",
    "The trend between two columns describes how they vary in relation to each other, for example the correlation. The higher the score, the more the trends are alike. In our case, we don't pay attention to this score since most of our columns are categorical and unrelated by nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4d7fb5-ca6a-4493-ace9-8c3fd81b5eed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "report.generate(train_data.sample(n=2000), synthetic_data, metadata.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df0ca1b-919b-4449-92cd-d9c388167307",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "report.get_properties()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f919a9e7-a790-4f97-85a2-faebf5a6f3a5",
   "metadata": {},
   "source": [
    "This score needs to be looked at on a per-attribute level as opposed to the overall score since a lot of columns do not make sense when 'synthesized', for example ID based columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a33b1d3-d550-45e2-bb3f-ef3ca5b2c976",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "report.get_details(property_name='Column Shapes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0af13b-d9d2-4730-93b2-2cffa96a7fc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "report.save(filepath='results/quality_report_mar_04.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b3a96c-3494-4873-869a-cd98e4b0efc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sdmetrics.reports.single_table import QualityReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfe6596-f346-4acf-9240-128bbf56f676",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# quality_report = QualityReport.load('results/quality_report_feb_22.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa64e5d2-f9ac-43f6-80d2-b84a1a494e66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = report.get_visualization(property_name='Column Shapes')\n",
    "\n",
    "# Update the title of the plot\n",
    "fig.update_layout(title='Data Quality for Gaussian Copula: Column Shapes (Average Score = 0.74)')\n",
    "\n",
    "# Find the index of the trace corresponding to TVComplement\n",
    "tvcomplement_index = None\n",
    "for i, trace in enumerate(fig.data):\n",
    "    if trace.name == 'TVComplement':\n",
    "        tvcomplement_index = i\n",
    "        break\n",
    "\n",
    "# Update the color of the bars represented by TVComplement\n",
    "if tvcomplement_index is not None:\n",
    "    fig.data[tvcomplement_index].marker.color = '#00e1c9'\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a26db70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f25b61c6-1b85-4db0-b5f7-87d975746a3e",
   "metadata": {},
   "source": [
    "### Conditional synthetic data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ae4bf-9284-488a-a16b-063bb26bfca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7f1438-c04a-49dd-8694-aa4afce4c6f3",
   "metadata": {},
   "source": [
    "#### Model to predict number of transactions based on day, month, disk capacity and container_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4633fdff-909d-41a9-a113-6c2fe515f999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataModel:\n",
    "    def predict(self, day, month, disk_capacity, container_group):\n",
    "        container_group_encoded = self.label_encoder.transform([container_group])[0]\n",
    "        # Prepare input for prediction\n",
    "        sample_input = pd.DataFrame({\n",
    "            'disk_capacity_tb': [disk_capacity],\n",
    "            'container_group_encoded': [container_group_encoded],\n",
    "            'month': [month],\n",
    "            'day': [day],\n",
    "            'day_sin': [np.sin(day * (2. * np.pi / 31))],\n",
    "            'day_cos': [np.cos(day * (2. * np.pi / 31))],\n",
    "            'month_sin': [np.sin((month - 1) * (2. * np.pi / 12))],\n",
    "            'month_cos': [np.cos((month - 1) * (2. * np.pi / 12))]\n",
    "        })\n",
    "\n",
    "        return int(self.model.predict(sample_input)[0])\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663e4a18-2af0-42d6-a611-e8410d61e92a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('SamplePredictor.pickle', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2016123e-1bdc-4b62-9672-1affa6a0fcb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8a6b487-90ba-45fa-83b8-0ce6504a6004",
   "metadata": {},
   "source": [
    "#### Condition generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cb3025-5fa3-4632-834b-e7a56699c4b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_conditions(start_date, end_date, disk_capacity, container_groups):\n",
    "    conditions = []\n",
    "    \n",
    "    current_date = start_date\n",
    "    delta = timedelta(days=1)\n",
    "    \n",
    "    cg = dict()\n",
    "    for container_group, container_id_types in container_groups.items():\n",
    "        cg[container_group] = random.sample(container_dict[container_group], container_id_types)\n",
    "    \n",
    "    while current_date <= end_date:\n",
    "        for container_group in container_groups.keys():\n",
    "            num_rows = model.predict(day=current_date.day, month=current_date.month, disk_capacity=disk_capacity, container_group=container_group)\n",
    "            for container_id in cg[container_group]:\n",
    "                conditions.extend([Condition(num_rows=int(num_rows//len(cg[container_group])), \n",
    "                                         column_values={'datetime_column': current_date, \n",
    "                                                        'container_group': container_group,\n",
    "                                                        'container_id': container_id\n",
    "                                                       }\n",
    "                                        )])\n",
    "        current_date += delta\n",
    "    return conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7518c6d4-9d6f-4be8-a092-43b3b105f727",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(train_data['container_group'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ee7159-4e6f-49b3-9564-feb1a8b5db3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data['datetime_column'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcb1c35-a702-410b-9571-12e558314a5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# defining variables to be passed to the synthesizer\n",
    "start_date = datetime.strptime('2022-01-27', '%Y-%m-%d')\n",
    "end_date = datetime.strptime('2022-01-28', '%Y-%m-%d')\n",
    "disk_capacity_tb = 20 # Numberical Value\n",
    "# this translates to I want 2 container_ids of type X and 3 container_ids of type Y\n",
    "container_groups = {'X':2, 'Y':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1ce4d2-7c07-4fa9-8ec3-192f455ed241",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conditions = generate_conditions(start_date, end_date, disk_capacity, container_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4538da17-d5b8-4255-9631-942f1931fc38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conditional_synthetic_data = synthesizer.sample_from_conditions(conditions=conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6991568a-d2ca-48ea-b77c-1048fc54f6c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conditional_synthetic_data['disk_capacity_tb'] = disk_capacity_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a7d807-8855-4d3e-ba8c-1886659c8bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conditional_synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd23ff6a-3c53-4455-8fba-1d052ba6625c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d74f13b-fc24-4d90-966d-8466914cb262",
   "metadata": {},
   "source": [
    "### Experimented with different models that predict the number of transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1dbcc0-b003-40dd-bb10-7e99a8dbdb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200be600-1257-4bb5-89ce-8f8862eba2a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = train_data\n",
    "df['month'] = df['datetime_column'].dt.month\n",
    "df['day'] = df['datetime_column'].dt.day\n",
    "\n",
    "agg_data = df.groupby(['day', 'month', 'container_group', 'disk_capacity_tb']).size().reset_index(name='transactions')\n",
    "\n",
    "xgb_model = XGBRegressor(n_estimators=320, learning_rate=0.009, max_depth=10, random_state=42)\n",
    "\n",
    "# label encoding for categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "agg_data['container_group_encoded'] = label_encoder.fit_transform(agg_data['container_group'])\n",
    "# agg_data['operation_encoded'] = label_encoder.fit_transform(agg_data['operation'])\n",
    "\n",
    "X = agg_data[['day', 'month', 'container_group_encoded', 'disk_capacity_tb']]\n",
    "y = agg_data['transactions']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n",
    "\n",
    "# standardize features \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# fit the model\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# model evaluation\n",
    "y_pred = xgb_model.predict(X_test_scaled)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'Mean Absolute Error = {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb3880a-625a-4a9f-8b15-3136f87db16f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'min_child_weight': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='neg_mean_absolute_error', cv=5, verbose=1)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model using Mean Squared Error\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'Mean Absolute Error on Test Set: {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e36dd37-6505-4b22-9d52-d1d061be5057",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample input\n",
    "new_input = [6,10,4,20]\n",
    "new_input_scaled = scaler.transform([new_input])\n",
    "\n",
    "# making a prediction\n",
    "prediction = xgb_model.predict(new_input_scaled)\n",
    "print(f'Predicted number of rows = {abs(int(prediction[0]))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdd8ec6-6c80-4ecf-a5e3-4ee87c33cdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "class DataModel:\n",
    "    def predict(self, day, month, disk_capacity, container_group):\n",
    "        container_group_encoded = self.label_encoder.transform([container_group])[0]\n",
    "        # Prepare input for prediction\n",
    "        sample_input = pd.DataFrame({\n",
    "            'disk_capacity_tb': [disk_capacity],\n",
    "            'container_group_encoded': [container_group_encoded],\n",
    "            'month': [month],\n",
    "            'day': [day],\n",
    "            'day_sin': [np.sin(day * (2. * np.pi / 31))],\n",
    "            'day_cos': [np.cos(day * (2. * np.pi / 31))],\n",
    "            'month_sin': [np.sin((month - 1) * (2. * np.pi / 12))],\n",
    "            'month_cos': [np.cos((month - 1) * (2. * np.pi / 12))]\n",
    "        })\n",
    "\n",
    "        return int(self.model.predict(sample_input)[0])\n",
    "    pass\n",
    "with open('SamplePredictor.pickle', 'rb') as f:\n",
    "    data_model = pickle.load(f)\n",
    "    print(data_model.predict(day=1, month=1, disk_capacity=20, container_group='02892102A8F17B5A551466B444222F4C3D9A399F'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9807305f-423e-413a-aced-edc1d02342a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e40acd7d-72b0-473b-be23-976aa6e5a393",
   "metadata": {},
   "source": [
    "### Validation of data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c717f299-ce9b-4cdc-b5b6-6ba9b54522a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c340d2b-4b25-49c1-9d59-dcab36ecb351",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataQValidation:\n",
    "\n",
    "    def __init__(self, original, synthetic, start_date, end_date, disk_capacity_tb, container_groups):\n",
    "        self.original_df = original\n",
    "        self.synthetic_df = synthetic\n",
    "\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.disk_cap = disk_capacity_tb\n",
    "        self.container_groups = container_groups\n",
    "\n",
    "        self.original_df['datetime_column'] = pd.to_datetime(self.original_df['datetime_column'], errors='coerce')\n",
    "        self.synthetic_df['datetime_column'] = pd.to_datetime(self.synthetic_df['datetime_column'], errors='coerce')\n",
    "\n",
    "        self.original_df = self.original_df.sort_values(by='datetime_column')\n",
    "        self.synthetic_df = self.synthetic_df.sort_values(by='datetime_column')\n",
    "\n",
    "    def unique_data(self):\n",
    "\n",
    "        def row_hash(row):\n",
    "            return hashlib.sha256(row.to_string().encode('utf-8')).hexdigest()\n",
    "\n",
    "        original_hashes = self.original_df.apply(lambda row: row_hash(row), axis=1)\n",
    "        synthetic_hashes = self.synthetic_df.apply(lambda row: row_hash(row), axis=1)\n",
    "\n",
    "        tqdm.pandas(desc=\"Hashing original rows\")\n",
    "        original_hashes = self.original_df.progress_apply(row_hash, axis=1)\n",
    "\n",
    "        tqdm.pandas(desc=\"Hashing synthetic rows\")\n",
    "        synthetic_hashes = self.synthetic_df.progress_apply(row_hash, axis=1)\n",
    "\n",
    "        original_set = set(original_hashes)\n",
    "        synthetic_set = set(synthetic_hashes)\n",
    "\n",
    "        exact_matches = original_set.intersection(synthetic_set)\n",
    "\n",
    "        print(\"Number of exact matches with original data:\", len(exact_matches))\n",
    "\n",
    "        print(\"Number of exact matches within synthetic data:\", len(synthetic_hashes) - len(synthetic_set))\n",
    "\n",
    "    def check_capacity(self):\n",
    "\n",
    "        cap_cumulative_test = True\n",
    "        positive_chunk = True\n",
    "\n",
    "        cumulative_sizes = {container_id: 0 for container_id in self.synthetic_df['container_id'].unique()}\n",
    "        fixed_disk_cap_gb = 16 * 1e9\n",
    "\n",
    "        for index, row in self.synthetic_df.iterrows():\n",
    "\n",
    "            if row['chunk_size'] < 0:\n",
    "                positive_chunk = False\n",
    "\n",
    "            if row['operation'] == 'WRITE':\n",
    "                cumulative_sizes[row['container_id']] += row['chunk_size']\n",
    "\n",
    "            if row['operation'] == 'DELETE_PERFORMED':\n",
    "                cumulative_sizes[row['container_id']] -= row['chunk_size']\n",
    "\n",
    "            if cumulative_sizes[row['container_id']] > fixed_disk_cap_gb: # Assuming we get 100% of the theoretical space\n",
    "                cap_cumulative_test = False\n",
    "\n",
    "        if cap_cumulative_test:\n",
    "            print(\"Cumulative Data Test Passed : Size did not exceed disk capacity for any container at any given time\")\n",
    "        else:\n",
    "            print(\"Cumulative Data Test Failed : Size did exceeded disk capacity for some container at a given time\")\n",
    "\n",
    "        if positive_chunk:\n",
    "            print(\"Positive chunk size test passed\")\n",
    "        else:\n",
    "            print(\"Positive chunk size test failed\")\n",
    "            \n",
    "        print(cumulative_sizes)\n",
    "        print(fixed_disk_cap_gb)\n",
    "\n",
    "    def null_check(self):\n",
    "        if self.synthetic_df.isnull().values.any():\n",
    "            print(\"Null Test Failed : Null values found in the generated data\")\n",
    "        else:\n",
    "            print(\"Null Test Passed\")\n",
    "\n",
    "    def range_check(self):\n",
    "\n",
    "        COUNT_THRESHOLD = 3\n",
    "\n",
    "        container_group_master = self.original_df['container_group'].unique()\n",
    "\n",
    "        operations = set(self.synthetic_df['operation'])\n",
    "        ops_master = set(self.original_df['operation'])\n",
    "        if not operations.issubset(ops_master):\n",
    "            print('Operation Range Test Failed : Unkown Operation found')\n",
    "        else:\n",
    "            print('Operation Range Test Passed')\n",
    "\n",
    "        print('\\n')\n",
    "        if self.disk_cap:\n",
    "            disk_caps = set(self.synthetic_df['disk_capacity_tb'])\n",
    "            if len(disk_caps) == 1:\n",
    "                if next(iter(disk_caps)) == self.disk_cap:\n",
    "                    print(\"Disk Capacity Range Test Passed : Synthetic data generated for the conditioned disk capacity\")\n",
    "                else:\n",
    "                    print(\"Disk Capacity Range Test Failed : Synthetic data generated has different disk capacity than the condition\")\n",
    "            else:\n",
    "                print(\"Disk Capacity Range Test Failed : 0 or more than 1 unique disk capacities found in synthetic data\")\n",
    "\n",
    "        print('\\n')\n",
    "        if self.container_groups:\n",
    "            if not set(self.container_groups.keys()).issubset(container_group_master):\n",
    "                print(\"Container Group Range Test cannot be perfomed, synthetic data generated on different container groups than original data\")\n",
    "            else:\n",
    "                synthetic_cg_master = self.synthetic_df['container_group'].unique()\n",
    "                if not set(self.container_groups.keys()).issubset(synthetic_cg_master):\n",
    "                    print(\"Container Group Range Test Failed : synthetic data is missing some conatiner groups\")\n",
    "                elif not set(synthetic_cg_master).issubset(self.container_groups.keys()):\n",
    "                    print(\"Container Group Range Test Failed : synthetic data has extra conatiner groups not passed in condition\")\n",
    "                else:\n",
    "                    print(\"Container Group Range Test Passed : synthetic data and conditions have exact container groups\")\n",
    "\n",
    "        synthetic_cg_metadf = self.synthetic_df.groupby(['container_group', 'container_id'])['chunk_id'].count().reset_index()\n",
    "        transaction_counts = dict(synthetic_cg_metadf.groupby('container_group')['chunk_id'].sum())\n",
    "        container_counts = dict(synthetic_cg_metadf.groupby('container_group')['container_id'].count())\n",
    "\n",
    "        predictor = self.load_predictor_model()\n",
    "        delta = timedelta(days=1)\n",
    "\n",
    "        transactions_test = True\n",
    "        container_count_test = True\n",
    "        total_transactions = 0\n",
    "        for cg, ccount in self.container_groups.items():\n",
    "            current_date = self.start_date\n",
    "            predicted_count = 0\n",
    "            while current_date <= end_date:\n",
    "                predicted_count += predictor.predict(day=current_date.day, month=current_date.month, disk_capacity= self.disk_cap, container_group= cg)\n",
    "                current_date += delta\n",
    "\n",
    "            if abs(predicted_count - transaction_counts[cg]) > COUNT_THRESHOLD:\n",
    "                transactions_test = False\n",
    "\n",
    "            if ccount != container_counts[cg]:\n",
    "                container_count_test = False\n",
    "\n",
    "        print('\\n')\n",
    "        if transactions_test:\n",
    "            print('Transaction Number Test Passed : Number of samples generated is in lieu with models prediction')\n",
    "        else:\n",
    "            print('Transaction Number Test Failed : Number of samples generated is not in lieu with models prediction')\n",
    "\n",
    "        print('\\n')\n",
    "        if container_count_test:\n",
    "            print('Container Count Test Passed : Number of unique containers found in synthetic data is equal to condition')\n",
    "        else:\n",
    "            print('Container Count Test Failed : Number of unique containers found in synthetic data is not equal to condition')\n",
    "\n",
    "        containers = list(synthetic_cg_metadf['container_id'])\n",
    "\n",
    "        lid_test = True\n",
    "        sid_test = True\n",
    "        cid_test = True\n",
    "        did_test = True\n",
    "\n",
    "        for co in containers:\n",
    "            uog = self.original_df[self.original_df['container_id'] == co]\n",
    "            uog_lid = set(uog['location_id'].unique())\n",
    "            uog_sid = set(uog['server_id'].unique())\n",
    "            uog_cid = set(uog['config_id'].unique())\n",
    "            uog_did = set(uog['disk_id'].unique())\n",
    "\n",
    "            us = self.synthetic_df[self.synthetic_df['container_id'] == co]\n",
    "            us_lid = set(us['location_id'].unique())\n",
    "            us_sid = set(us['server_id'].unique())\n",
    "            us_cid = set(us['config_id'].unique())\n",
    "            us_did = set(us['disk_id'].unique())\n",
    "\n",
    "            if not us_lid.issubset(uog_lid):\n",
    "                lid_test = False\n",
    "            if not us_sid.issubset(uog_sid):\n",
    "                sid_test = False\n",
    "            if not us_cid.issubset(uog_cid):\n",
    "                cid_test = False\n",
    "            if not us_did.issubset(uog_did):\n",
    "                did_test = False\n",
    "\n",
    "        print('\\n')\n",
    "        if lid_test:\n",
    "            print('Location Uniqueness Test Passed : Container in synthetic data and original data belong to same location id')\n",
    "        else:\n",
    "            print('Location Uniqueness Test Failed : Container in synthetic data and original data do not belong to same location id')\n",
    "\n",
    "        print('\\n')\n",
    "        if sid_test:\n",
    "            print('Server Uniqueness Test Passed : Container in synthetic data and original data belong to same server id')\n",
    "        else:\n",
    "            print('Server Uniqueness Test Failed : Container in synthetic data and original data do not belong to same server id')\n",
    "\n",
    "        print('\\n')\n",
    "        if cid_test:\n",
    "            print('Config Uniqueness Test Passed : Container in synthetic data and original data belong to same config id')\n",
    "        else:\n",
    "            print('Config Uniqueness Test Failed : Container in synthetic data and original data do not belong to same config id')\n",
    "\n",
    "        print('\\n')\n",
    "        if did_test:\n",
    "            print('Disk Uniqueness Test Passed : Container in synthetic data and original data belong to same disk id')\n",
    "        else:\n",
    "            print('Disk Uniqueness Test Failed : Container in synthetic data and original data do not belong to same disk id')\n",
    "\n",
    "        outside_range = self.synthetic_df[(self.synthetic_df['datetime_column'] < self.start_date) | (self.synthetic_df['datetime_column'] > self.end_date)]\n",
    "\n",
    "        print('\\n')\n",
    "        if outside_range.empty:\n",
    "            print('Date Range Test Passed : All dates are within the start and end date')\n",
    "        else:\n",
    "            print('Date Range Test Failed : Some dates are outside start and end date')\n",
    "\n",
    "    def load_predictor_model(self):\n",
    "        class DataModel:\n",
    "            def predict(self, day, month, disk_capacity, container_group):\n",
    "                container_group_encoded = self.label_encoder.transform([container_group])[0]\n",
    "                # Prepare input for prediction\n",
    "                sample_input = pd.DataFrame({\n",
    "                    'disk_capacity_tb': [disk_capacity],\n",
    "                    'container_group_encoded': [container_group_encoded],\n",
    "                    'month': [month],\n",
    "                    'day': [day],\n",
    "                    'day_sin': [np.sin(day * (2. * np.pi / 31))],\n",
    "                    'day_cos': [np.cos(day * (2. * np.pi / 31))],\n",
    "                    'month_sin': [np.sin((month - 1) * (2. * np.pi / 12))],\n",
    "                    'month_cos': [np.cos((month - 1) * (2. * np.pi / 12))]\n",
    "                })\n",
    "\n",
    "                return int(self.model.predict(sample_input)[0])\n",
    "            pass\n",
    "        with open('SamplePredictor.pickle', 'rb') as f:\n",
    "            data_model = pickle.load(f)\n",
    "        return data_model\n",
    "\n",
    "    def stat_dist(self, col, isCat=False):\n",
    "        if isCat:\n",
    "            print(f\"Original {col} distribution:\\n\", self.original_df[col].value_counts(normalize=True))\n",
    "            print(f\"Synthetic {col} distribution:\\n\", self.synthetic_df[col].value_counts(normalize=True))\n",
    "        else:\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "            # Plotting histograms\n",
    "            sns.histplot(self.original_df[col], color=\"skyblue\", label='Original', kde=True, ax=ax)\n",
    "            sns.histplot(self.synthetic_df[col], color=\"red\", label='Synthetic', kde=True, ax=ax, alpha=0.6)\n",
    "\n",
    "            plt.legend()\n",
    "            plt.title(f'Distribution of {col}')\n",
    "\n",
    "            # Summary statistics\n",
    "            original_stats = self.original_df[col].describe()\n",
    "            synthetic_stats = self.synthetic_df[col].describe()\n",
    "\n",
    "            stats_df = pd.DataFrame({'Original': original_stats, 'Synthetic': synthetic_stats})\n",
    "            stats_text = stats_df.to_string()\n",
    "\n",
    "            # Adding text box for summary statistics\n",
    "            plt.text(1.05, 0.95, stats_text, transform=ax.transAxes, fontsize=10, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "    def unique_data_by_col(self, col):\n",
    "        print(f\"Unique values in original data {col}:\", self.original_df[col].nunique())\n",
    "        print(f\"Unique values in synthetic data {col}:\", self.synthetic_df[col].nunique())\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_datetime(df):\n",
    "        df['datetime_column'] = pd.to_datetime(df['datetime_column'], format='%Y-%m-%d %H:%M:%S')\n",
    "        return df\n",
    "\n",
    "    def compare_numerical_distributions(self, col):\n",
    "        \"\"\"\n",
    "        Compare the distributions of a numerical column using histograms and the Kolmogorov-Smirnov test.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.histplot(self.original_df[col], color=\"skyblue\", label=\"Original\", kde=True, stat=\"density\", bins=30)\n",
    "        sns.histplot(self.synthetic_df[col], color=\"red\", label=\"Synthetic\", kde=True, stat=\"density\", bins=30)\n",
    "        plt.legend()\n",
    "        plt.title(f'Distribution Comparison for {col}')\n",
    "        plt.show()\n",
    "\n",
    "        stat, p = ks_2samp(self.original_df[col], self.synthetic_df[col])\n",
    "        print(f\"Kolmogorov-Smirnov test for {col}: Statistic={stat:.4f}, P-value={p:.4g}\")\n",
    "\n",
    "    def compare_categorical_distributions(self, col):\n",
    "        \"\"\"\n",
    "        Compare the distributions of a categorical column using count plots on the same graph\n",
    "        for both original and synthetic datasets.\n",
    "        :param col: The column name for the categorical data.\n",
    "        \"\"\"\n",
    "        original_df_copy = self.original_df.copy()\n",
    "        synthetic_df_copy = self.synthetic_df.copy()\n",
    "        original_df_copy['Dataset'] = 'Original'\n",
    "        synthetic_df_copy['Dataset'] = 'Synthetic'\n",
    "\n",
    "        combined_df = pd.concat([original_df_copy, synthetic_df_copy], ignore_index=True)\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.countplot(x=col, hue='Dataset', data=combined_df, palette='viridis')\n",
    "        plt.title(f'Comparison of {col} Distribution Between Original and Synthetic Data')\n",
    "        plt.xticks(rotation=45)  # Rotate the x-axis labels for better readability\n",
    "        plt.legend(title='Dataset')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def temporal_distribution_check(self):\n",
    "        \"\"\"\n",
    "        Validate the consistency of event distribution over time.\n",
    "        \"\"\"\n",
    "        original_timeseries = self.original_df.set_index('datetime_column').resample('M').size()\n",
    "        synthetic_timeseries = self.synthetic_df.set_index('datetime_column').resample('M').size()\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        original_timeseries.plot(label='Original', color='blue')\n",
    "        synthetic_timeseries.plot(label='Synthetic', color='red')\n",
    "        plt.legend()\n",
    "        plt.title('Temporal Distribution Comparison')\n",
    "        plt.xlabel('Month')\n",
    "        plt.ylabel('Count')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_cdf(self, col):\n",
    "        \"\"\"\n",
    "        Plots the CDF for a numerical column for both original and synthetic datasets.\n",
    "        :param col: The column name to analyze.\n",
    "        \"\"\"\n",
    "        x_original = np.sort(self.original_df[col])\n",
    "        y_original = np.arange(1, len(x_original)+1) / len(x_original)\n",
    "        x_synthetic = np.sort(self.synthetic_df[col])\n",
    "        y_synthetic = np.arange(1, len(x_synthetic)+1) / len(x_synthetic)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(x_original, y_original, marker='.', linestyle='none', label='Original')\n",
    "        plt.plot(x_synthetic, y_synthetic, marker='.', linestyle='none', label='Synthetic')\n",
    "        plt.legend()\n",
    "        plt.title(f'Cumulative Distribution Function (CDF) of {col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('CDF')\n",
    "        plt.show()\n",
    "\n",
    "    def run_test_suite(self):\n",
    "        self.null_check()\n",
    "        print('\\n')\n",
    "        print('-'*50)\n",
    "        self.stat_dist(col='chunk_size')\n",
    "        print('\\n')\n",
    "        print('-'*50)\n",
    "        self.unique_data()\n",
    "        print('\\n')\n",
    "        print('-'*50)\n",
    "        self.range_check()\n",
    "        print('\\n')\n",
    "        print('-'*50)\n",
    "        self.check_capacity()\n",
    "        print('\\n')\n",
    "        print('-'*50)\n",
    "        self.compare_numerical_distributions(col='chunk_size')\n",
    "        print('\\n')\n",
    "        print('-'*50)\n",
    "        self.plot_cdf(col='chunk_size')\n",
    "\n",
    "validation = DataQValidation(train_data, conditional_synthetic_data, start_date, end_date, disk_capacity_tb, container_groups)\n",
    "validation.run_test_suite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f81c61a-aa77-4d90-b658-8e785a15eed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
